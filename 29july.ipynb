{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d593f9",
   "metadata": {},
   "source": [
    "Youtube title extraction \n",
    "\n",
    "\n",
    "In this file trying to fetch youtube data and filtering out all unncecessary parameter from the youtube.\n",
    "importing the important libraray, you data  is used to get data from Youtube Data API\n",
    "this recommender system will be able to recommend videos to us, based on top news topics and\n",
    "based on combination of features.\n",
    "First, we load the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c893cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import re \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib.pyplot import xlabel,ylabel,title\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tweepy\n",
    "from tweepy import API\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c5491",
   "metadata": {},
   "source": [
    "1. Gather the data\n",
    " import data from Youtube Data API\n",
    "authentication details to connect with twittet api to fetch data.\n",
    "For this problem I decided to retrieve the dataset based on top twitter topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f20d1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672\n",
      "67097\n",
      "youtube#video\n",
      "Europe struggles with major wildfires and energy uncertainty amid 'heat apocalypse'\n",
      "2760\n",
      "310637\n",
      "youtube#video\n",
      "Massive California wildfire erupts\n",
      "290\n",
      "31601\n",
      "youtube#video\n",
      "Record heat in the Northwest as wildfires continue l GMA\n",
      "53\n",
      "4045\n",
      "youtube#video\n",
      "Wildfire In Texas Scorches 500 Acres As Crews Continue To Battle The Blaze\n",
      "44\n",
      "8300\n",
      "youtube#video\n",
      "Raw: New wildfire near Colfax prompts evacuation order\n"
     ]
    }
   ],
   "source": [
    "title = []\n",
    "viewcount = []\n",
    "likecount = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "youtube = build('youtube', 'v3', developerKey='AIzaSyBfoGn0960ZupAD7YiIdwfRe1MDbdg9F_U')\n",
    "\n",
    "    \n",
    "video_statistics= youtube.videos().list(id =['WluvF8Tj5tc','chZp2U09Qa8','frJV7qx9v6Y','KsnjW-sLNTo','FUQXSz4yV-w'],\n",
    "                                        part=['statistics','snippet']).execute()\n",
    "#print(video_statistics)\n",
    "\n",
    "for i in range(5):\n",
    "    likecount = int(video_statistics['items'][i]['statistics']['likeCount'])\n",
    "    viewcount = int(video_statistics['items'][i]['statistics']['viewCount'])\n",
    "    video_kind = video_statistics['items'][i]['kind']\n",
    "   \n",
    "    title = video_statistics['items'][i]['snippet']['title']\n",
    "    print(likecount)\n",
    "    print(viewcount)\n",
    "    print(video_kind)\n",
    "\n",
    "    print(title)\n",
    "  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025ae23",
   "metadata": {},
   "source": [
    "first content-based recommender will have a goal of recommending videos which have a similar plot to a selected tweet topics.\n",
    "searching th youtube title based on particular video id and extracting the youtube  data, like count, viewcount,title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67553c15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'youtube_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m video_kind \u001b[38;5;241m=\u001b[39m video_statistics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     40\u001b[0m title \u001b[38;5;241m=\u001b[39m video_statistics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 41\u001b[0m \u001b[43myoutube_data\u001b[49m[title] \u001b[38;5;241m=\u001b[39m viewcount\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(likecount)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(viewcount)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'youtube_data' is not defined"
     ]
    }
   ],
   "source": [
    "title1 = []\n",
    "viewcount1 = []\n",
    "likecount1 = []\n",
    "try:\n",
    "    os.remove('tweets1data.csv')\n",
    "    #print \"File deleted\"\n",
    "except:\n",
    "    #print \"File was not present already\"\n",
    "    pass\n",
    "\n",
    "csvFile = open('youtubedata.csv', 'w')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "youtube = build('youtube', 'v3', developerKey='AIzaSyBfoGn0960ZupAD7YiIdwfRe1MDbdg9F_U')\n",
    "\n",
    "    \n",
    "video_statistics= youtube.videos().list(id =['WluvF8Tj5tc','chZp2U09Qa8','frJV7qx9v6Y',\n",
    "                                             \"Rtjc3HIvFkY\",'KsnjW-sLNTo','FUQXSz4yV-w',\n",
    "                                             'RRhjqqe750A','Xkyq03pfYUs','Z5p0hhYmDFg',\n",
    "                                             'Zn7WhBZ8J-M','f60Z4epksOk','erinPN6glcE',\n",
    "                                             'I82qvaRmybM','lcUk1cYWY9I','aircAruvnKk',\n",
    "                                             't4B99T_3IsM','6Gmohrod2kY','2pBRZ29yWyY','Bgv1KInL2ek','5DGwOJXSxqg'],\n",
    "                                        part=['statistics','snippet',]).execute()\n",
    "\n",
    "\n",
    "\n",
    "#print(video_statistics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    likecount = int(video_statistics['items'][i]['statistics']['likeCount'])\n",
    "    viewcount = int(video_statistics['items'][i]['statistics']['viewCount'])\n",
    "    video_kind = video_statistics['items'][i]['kind']\n",
    "   \n",
    "    title = video_statistics['items'][i]['snippet']['title']\n",
    "    youtube_data[title] = viewcount\n",
    "    print(likecount)\n",
    "    print(viewcount)\n",
    "    print(video_kind)\n",
    "    title1.append(title)\n",
    "    likecount1.append(likecount)\n",
    "    viewcount1.append(viewcount)\n",
    "\n",
    "    print(title)\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9999d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_data = pd.DataFrame({'Title':title1,'View_Count':viewcount1,'Like_Count':likecount1}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a389e",
   "metadata": {},
   "source": [
    "creating data frames for youtube data using pandas with title, view_count,Like_count as header. printing all fetched youtube data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(youtube_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0cc364",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = pd.DataFrame(data = youtube_data, columns = ['title','viewcount'],index=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d901dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(youtube_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b83931",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f525c",
   "metadata": {},
   "source": [
    "this function removes all @ symbolic sentencs in title of th eyoutube data,removes the tags,remove capital letter which are not necessary. filtering out tweets will \n",
    "help to decrease the input data. increase the proccessing speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170508da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  text = re.sub(r'@[A-Za-z0-9]+','',text) #removes @mentions\n",
    "  text = re.sub(r'#','',text) #removes # tags\n",
    "  text = re.sub(r'\\\\n','',text) #removes # tags\n",
    "  text = re.sub(r'RT[\\s]+','',text) #removes RT\n",
    "  text = re.sub(r'https?:\\/\\/\\S+','',text) #removes urls\n",
    "\n",
    "  return text\n",
    "youtube_data['clean_text']=youtube_data['Title'].apply(clean_text)\n",
    "\n",
    "#display clean text\n",
    "youtube_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e600ce6",
   "metadata": {},
   "source": [
    "TF-IDF approach\n",
    "A statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "TF-IDF of a word in a document which is part of a larger corpus of documents is a combination of two values. One is term frequency (TF), which measures how frequently the word occurs in the document some of the words, such as “the” and “is”, occur frequently in all documents and we want to downscale the importance of such words. This is accomplished by multiplying TF with the inverse document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tf_idf_matrix = tf_idf.fit_transform(youtube_data['Title']);\n",
    "print(tf_idf)\n",
    "print(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a6afe",
   "metadata": {},
   "source": [
    "Cosine similarity\n",
    "By applying the definition of similarity, this will be in fact equal to 1 if the two vectors are identical, \n",
    "and it will be 0 if the two are orthogonal. In other words, the similarity is a number bounded between \n",
    "0 and 1 that tells us how much the two vectors are similar. \n",
    "Now that we have numerical vectors, representing each tweets plot description, here i tried can compute similarity of tweets content\n",
    "by calculating their pair-wise cosine similarities\n",
    "and storing them in cosine similarity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_matrix = cosine_similarity(tf_idf_matrix, tf_idf_matrix)\n",
    "print(cosine_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d09acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_from_title(df,title):\n",
    "    return df[df['Title']==title].index.values[0]\n",
    "\n",
    "\n",
    "# function that returns the title of the movie from its index\n",
    "\n",
    "def title_from_index(df,index):\n",
    "\n",
    "    return df[df.index==index].title.values[0]\n",
    "\n",
    "\n",
    "# generating recommendations for given title\n",
    "\n",
    "def recommendations( title, df,cosine_similarity_matrix,number_of_recommendations):\n",
    "\n",
    "    index = index_from_title(df,title)\n",
    "\n",
    "    similarity_scores = list(enumerate(cosine_similarity_matrix[index]))\n",
    "\n",
    "    similarity_scores_sorted = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    recommendations_indices = [t[0] for t in similarity_scores_sorted[1:(number_of_recommendations+1)]]\n",
    "\n",
    "    return df['title'].iloc[recommendations_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81377015",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations('stock', youtube_data, cosine_similarity_matrix, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256933a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea45f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
